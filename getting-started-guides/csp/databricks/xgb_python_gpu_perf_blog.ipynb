{"cells":[{"cell_type":"markdown","source":["## GPU based PySpark XGBoost"],"metadata":{}},{"cell_type":"markdown","source":["##### Importing XGBoost, hyperopt, scikit learn, pandas and other helper function packages"],"metadata":{}},{"cell_type":"code","source":["import xgboost as xgb\n\nfrom hyperopt import hp, fmin, tpe, STATUS_OK, SparkTrials\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\n\nimport numpy as np\nimport pandas as pd\n\nimport os\nimport shutil\nimport tempfile"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":["## Data Loading"],"metadata":{}},{"cell_type":"markdown","source":["##### For a large dataset, broadcasting the dataset would take significant cluster resources. We store the data on DBFS and load it back on workers via DBFS' local file interface.\n\nSee Databricks best practices for HyperOpt: https://docs.databricks.com/applications/machine-learning/automl/hyperopt/hyperopt-best-practices.html"],"metadata":{}},{"cell_type":"code","source":["def load(path):\n    \"\"\"\n    Loads saved data (a tuple of numpy arrays).\n    Refernce: https://docs.databricks.com/applications/machine-learning/automl/hyperopt/hyperopt-best-practices.html\n    \"\"\"\n    return list(np.load(path).values())\n    \ndef save_to_dbfs(data):\n    \"\"\"\n    Saves input data (a tuple of numpy arrays) to a temporary file on DBFS and returns its path.\n    Refernce: https://docs.databricks.com/applications/machine-learning/automl/hyperopt/hyperopt-best-practices.html\n    \"\"\"\n    # Save data to a local file first.\n    data_filename = \"data.npz\"\n    local_data_dir = tempfile.mkdtemp()\n    local_data_path = os.path.join(local_data_dir, data_filename)\n    np.savez(local_data_path, *data)\n    # Move it to DBFS, which is shared among cluster nodes.\n    dbfs_tmp_dir = \"/dbfs/ml/tmp/hyperopt\"\n    os.makedirs(dbfs_tmp_dir, exist_ok=True)\n    dbfs_data_dir = tempfile.mkdtemp(dir=dbfs_tmp_dir)  \n    dbfs_data_path = os.path.join(dbfs_data_dir, data_filename)  \n    shutil.move(local_data_path, dbfs_data_path)\n    return dbfs_data_path"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":["##### Preparing XGBoost Data"],"metadata":{}},{"cell_type":"code","source":["def prepare_xgb_data(data, id_col=\"Id\", label_col=\"Label\", test_size=0.2):\n    \"\"\"\n    Prepare data for xgboost training\n    \"\"\"\n    # Make sure last column is label, first col\n    data[label_col+\"Temp\"] = data[label_col]\n    data = data.drop([id_col, label_col], axis=1)\n    data.rename(columns={label_col+\"Temp\": label_col}, inplace=True)\n    \n    # Prepare data\n    X, y = data.iloc[:,:-1],data.iloc[:,-1]\n    data_dmatrix = xgb.DMatrix(data=X,label=y)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=21)\n    return X_train, X_test, y_train, y_test\n    \ndef get_raw_data(file_name, sample_size):\n    input_file_location = \"/dbfs/FileStore/tables/\" + file_name\n    pdf = pd.read_csv(input_file_location).dropna().sample(n=sample_size)\n    return pdf"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":["## XGBoost Regression with Hyperopt + Spark Trials"],"metadata":{}},{"cell_type":"code","source":["def objective(space):\n    \"\"\"\n    Train and search input space\n    \"\"\"\n    clf = xgb.XGBRegressor(objective ='reg:squarederror', \n                            n_estimators = int(space['n_estimators']),\n                            colsample_bytree = space['colsample_bytree'],\n                            learning_rate = space['learning_rate'],\n                            max_depth = int(space['max_depth']),\n                            alpha = space['alpha'],\n                            tree_method= space['tree_method']\n                          )\n    \n    # Load data\n    data = load(data_large_path)\n    X_train, X_test, y_train, y_test = data[0], data[1], data[2], data[3]\n    eval_set  = [(X_train, y_train), (X_test, y_test)]\n\n    # Train\n    clf.fit(X_train, y_train,\n            eval_set=eval_set, eval_metric=\"rmse\",\n            early_stopping_rounds=10,verbose=False)\n    \n    # Validate\n    pred = clf.predict(X_test)\n    mse_scr = mean_squared_error(y_test, pred)\n\n    return {'loss': mse_scr, 'status': STATUS_OK}\n\ndef run_hyperopt(df, treemethod, parallelism, max_evals):\n    \"\"\"\n    Run hyperopt and return best params\n    \"\"\"\n    # Hyperopt search space\n    space ={'max_depth': hp.quniform('max_depth', 4, 16, 1),\n            'alpha' : hp.uniform('alpha', 1, 10),\n            'colsample_bytree' : hp.uniform('colsample_bytree', 0.1, 1),\n            'learning_rate' : hp.uniform('learning_rate', 0.1, 1),\n            'n_estimators': hp.quniform('n_estimators', 25, 500, 25),\n            'tree_method': treemethod\n        }\n    if parallelism is None:\n        trials = SparkTrials()\n    else:\n        trials = SparkTrials(parallelism=parallelism)\n\n    # Hyperopt\n    best_param = fmin(fn=objective,\n                space=space,\n                algo=tpe.suggest,\n                max_evals=max_evals,\n                trials=trials)\n    print(best_param)\n    \n    return best_param"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":["## Train"],"metadata":{}},{"cell_type":"markdown","source":["##### Parallelism parameter is set \"2\" for 2 GPUs, which is effectively using 2 GPUs in parallel. Each new hyperparameter setting tested will be chosen based on previous results. Setting parallelism in between 1 and max_evals allows you to trade off scalability (getting results faster) and adaptiveness (sometimes getting better models). For GPU, is is advised to set number of GPUs used for training."],"metadata":{}},{"cell_type":"code","source":["# Dataset\nfile_name = \"your_file_name.csv\" # dataset file name\nid_col=\"unique_id_column_name\" # unique id for each row\nlabel_col=\"label_column_name\" # label column name\n\n# Load data\ndf = get_raw_data(file_name=file_name, sample_size=10000)\ndata_large = prepare_xgb_data(df, id_col=id_col, label_col=label_col, test_size=0.2)\ndata_large_path = save_to_dbfs(data_large)\n\n# Run training\nbest_param = run_hyperopt(df, treemethod='gpu_hist', parallelism=2, max_evals=10) # Set parallelism = Number of GPUs\n\n# Cleanup\nshutil.rmtree(data_large_path, ignore_errors=True)"],"metadata":{},"outputs":[],"execution_count":13}],"metadata":{"name":"xgb_python_gpu_perf_blog","notebookId":323},"nbformat":4,"nbformat_minor":0}
